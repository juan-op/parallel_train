{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r reqirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from lightgbm import LGBMRegressor\n",
    "from pandas.tseries.offsets import MonthBegin\n",
    "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función es para crear un dataset de prueba, con el número de países, productos y meses que le indiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(n_countries: int, n_products: int, n_months: int) -> pd.DataFrame:\n",
    "    countries = [f\"country_{i}\" for i in range(n_countries)]\n",
    "    products = [f\"product_{i}\" for i in range(n_products)]\n",
    "    country_list = np.repeat(countries, n_products * n_months)\n",
    "    product_list = np.tile(products, n_countries * n_months)\n",
    "    dates = pd.date_range(start=\"2018-01-01\", periods=n_months, freq=\"MS\")\n",
    "    sales = np.random.randint(100, 1001, size=len(country_list))\n",
    "\n",
    "    # Create the DataFrame\n",
    "    input_data = (\n",
    "        pd.DataFrame()\n",
    "        .assign(\n",
    "            country=country_list,\n",
    "            product=product_list,\n",
    "            date=pd.to_datetime(np.tile(dates, n_countries * n_products)),\n",
    "            sales=sales,\n",
    "        )\n",
    "        .sort_values([\"country\", \"product\", \"date\"])\n",
    "    )\n",
    "    print(f\"Total number of data points: {len(input_data):,}\")\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos las funciones de entrenamiento y predicción (secuencial y en paralelo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict(models: list, data: pd.DataFrame) -> pd.DataFrame:\n",
    "    warnings.filterwarnings(\"ignore\")  # Ignoramos los warnings de skforecast\n",
    "    # Predecimos 6 meses en el futuro\n",
    "    prediction_horizon = 6\n",
    "\n",
    "    # Creamos un diccionario para guardar las predicciones.\n",
    "    predictions_dict = {\n",
    "        \"country\": [],\n",
    "        \"product\": [],\n",
    "        \"model\": [],\n",
    "        \"date\": [],\n",
    "        \"prediction\": [],\n",
    "    }\n",
    "\n",
    "    # Agrupamos los datos por país y producto para luego iterar sobre los grupos.\n",
    "    grouped = data.groupby([\"country\", \"product\"])\n",
    "\n",
    "    # Iteramos sobre los modelos y los grupos de datos, entrenamos cada modelo para cada grupo.\n",
    "    # Luego hacemos las predicciones y las guardamos en el diccionario.\n",
    "    for model in models:\n",
    "        for (country, product), group in grouped:\n",
    "            forecaster = ForecasterAutoreg(regressor=model, lags=4)\n",
    "            forecaster.fit(y=group[\"sales\"])\n",
    "            predictions = forecaster.predict(steps=prediction_horizon)\n",
    "\n",
    "            # Generamos las fechas de las predicciones\n",
    "            prediction_dates = [group[\"date\"].max() + MonthBegin(i) for i in range(1, prediction_horizon + 1)]\n",
    "\n",
    "            for date, predicted_sales in zip(prediction_dates, predictions):\n",
    "                predictions_dict[\"country\"].append(country)\n",
    "                predictions_dict[\"product\"].append(product)\n",
    "                predictions_dict[\"model\"].append(type(model).__name__)\n",
    "                predictions_dict[\"date\"].append(date)\n",
    "                predictions_dict[\"prediction\"].append(predicted_sales)\n",
    "\n",
    "    # Convertimos el diccionario en un DataFrame\n",
    "    predictions_dataframe = pd.DataFrame(predictions_dict)\n",
    "\n",
    "    return predictions_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_train(input_data: pd.DataFrame, models: list) -> float:\n",
    "    start_time = time.perf_counter()\n",
    "    results_df = train_predict(models, input_data)\n",
    "    end_time = time.perf_counter()\n",
    "    # Para el benchmark solo nos interesa el tiempo de ejecución\n",
    "    time_elapsed = end_time - start_time\n",
    "    return time_elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_train_auto(input_data: pd.DataFrame, models: list, n_cores: int) -> float:\n",
    "    # Dividimos el DataFrame en grupos por país, en base al número de cores,\n",
    "    # mapeamos cada país a un grupo para que todos los productos de un mismo país\n",
    "    # siempre estén en el mismo grupo.\n",
    "    country_list = input_data[\"country\"].unique()\n",
    "    country_to_group = {country: i % n_cores for i, country in enumerate(country_list)}\n",
    "    input_data[\"group\"] = input_data[\"country\"].map(country_to_group)\n",
    "    dataframes = [group.drop(columns=\"group\") for _, group in input_data.groupby(\"group\")]\n",
    "\n",
    "    # Comprobamos que el número de grupos sea igual al número de cores.\n",
    "    assert len(dataframes) == n_cores, \"The number of dataframes must match the number of cores.\"\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    # Luego usamos Parallel de joblib para entrenar los modelos en paralelo.\n",
    "    predictions = Parallel(n_jobs=n_cores)(delayed(train_predict)(models, data) for data in dataframes)\n",
    "\n",
    "    # Finalmente concatenamos los resultados en un solo DataFrame.\n",
    "    predictions_dataframe = pd.concat([result for result in predictions])\n",
    "    end_time = time.perf_counter()\n",
    "\n",
    "    # Para el benchmark solo nos interesa el tiempo de ejecución\n",
    "    time_elapsed = end_time - start_time\n",
    "    return time_elapsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la lista de modelos, se pueden añadir o quitar si quieres probar distintas combinaciones. Para los modelos que acepten `n_jobs` es mejor fijarlo en 1 para evitar problemas cuando se usan muchos cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\n",
    "    LinearRegression(),\n",
    "    GradientBoostingRegressor(random_state=42),\n",
    "    XGBRegressor(random_state=42, n_jobs=1),\n",
    "    LGBMRegressor(random_state=42, verbosity=-1, n_jobs=1),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutamos la comparativa. Recomiendo nunca usar todos los cores disponibles, puede dar problemas en la ejecución, mejor dejar siempre uno o dos cores de margen.\n",
    "\n",
    "Importante: el número de países tiene que ser igual o superior al número de cores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data points: 60,000\n",
      "Models: LinearRegression, GradientBoostingRegressor, XGBRegressor, LGBMRegressor\n",
      "--- Mean Execution Time ---\n",
      "Sequential: 55.47 seconds\n",
      "Parallel (2 cores): 28.18 seconds\n",
      "Parallel (4 cores): 15.38 seconds\n",
      "Parallel (6 cores): 11.58 seconds\n",
      "Parallel (8 cores): 10.48 seconds\n"
     ]
    }
   ],
   "source": [
    "input_data = prepare_data(n_countries=50, n_products=20, n_months=60)\n",
    "\n",
    "print(f\"Models: {', '.join([type(model).__name__ for model in MODELS])}\")\n",
    "print(\"--- Mean Execution Time ---\")\n",
    "\n",
    "time_sequential = sequential_train(models=MODELS, input_data=input_data)\n",
    "print(f\"Sequential: {time_sequential:0.2f} seconds\")\n",
    "\n",
    "# Ejecutamos el benchmark para el número de cores que queramos probar.\n",
    "for n_cores in [2, 4, 6, 8]:\n",
    "    time_parallel = parallel_train_auto(models=MODELS, input_data=input_data, n_cores=n_cores)\n",
    "    print(f\"Parallel ({n_cores} cores): {time_parallel:0.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parallel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
