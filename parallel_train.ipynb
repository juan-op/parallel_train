{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a23105da",
   "metadata": {},
   "source": [
    "## Paralelizar el entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f23709",
   "metadata": {},
   "source": [
    "La idea es dividir los datos por países y entrenar cada grupo de países en paralelo. Si por ejemplo tienes 100 países, divides los datos en 4 grupos de países y entrenas todos los modelos para todos los productos de cada grupo en paralelo. \n",
    "\n",
    "A cada grupo se le asigna un core. La clave es encontrar el número óptimo de grupos y cores ya que no siempre aumentando el número de grupos y cores el entrenamiento va a ir más rápido (puede incluso ir bastante más lento), es un equilibrio entre tener grupos suficientemente grandes y añadir más cores para paralelizarlo más."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fab9fb",
   "metadata": {},
   "source": [
    "\n",
    "He hecho algunas pruebas con varios escenarios para que te hagas una idea de qué puede funcionar mejor dependiendo del volumen de datos que tengas:\n",
    "\n",
    "- 50 países / 36 productos por país -> 6 grupos/cores.\n",
    "- 50 países / 50 productos por país -> 6 grupos/cores, aunque también se podrían considerar 8.\n",
    "- 70 países / 500 productos por país -> 8 grupos/cores.\n",
    "- 100 países / 100 productos por país -> 10 grupos/cores\n",
    "\n",
    "Las pruebas las he hecho con los siguientes modelos: LinearRegression, RandomForestRegressor (este es el modelo más lento con diferencia), GradientBoostingRegressor, XGBRegressor, LGBMRegressor.\n",
    "\n",
    "Ten en cuenta que si tienes un volumen de datos todavía mayor (+100 países / +100 productos por país) puede que usar más de 10 cores sea factible, pero en mi PC no lo he podido probar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ef367f",
   "metadata": {},
   "source": [
    "Para paralelizar el entrenamiento puedes usar:\n",
    "\n",
    "- **Multiprocessing**: https://docs.python.org/3.9/library/multiprocessing.html\n",
    "- **Joblib**: https://joblib.readthedocs.io/en/latest/parallel.html\n",
    "\n",
    "En este caso he optado por usar joblib, ya que es más sencillo de usar (acuérdate de `pip install joblib`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ea08e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from lightgbm import LGBMRegressor\n",
    "from pandas.tseries.offsets import MonthBegin\n",
    "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c325dc8",
   "metadata": {},
   "source": [
    "Esta es la función de entrenamiento. Le pasas la lista de modelos y el subset de los datos de cada grupo de países. Los datos se agrupan por país y producto y se itera por cada modelo y grupo, añadiendo las predicciones a un diccionario que luego se convierte en DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7616c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict(models: list, data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Trains multiple models on the given data and makes predictions.\n",
    "    The data is grouped by country and product, and each model is trained on each group.\n",
    "    The predictions are stored in a DataFrame along with the model name, country, product,\n",
    "    and prediction date.\n",
    "\n",
    "    Args:\n",
    "        models: A list of models to be trained.\n",
    "        data: A DataFrame containing the data to train the models on.\n",
    "\n",
    "    Returns:\n",
    "        A DataFrame containing the predictions.\n",
    "    \"\"\"\n",
    "    # Predecimos 6 meses en el futuro\n",
    "    prediction_horizon = 6\n",
    "\n",
    "    # Creamos un diccionario para guardar las predicciones.\n",
    "    predictions = {\n",
    "        \"country\": [],\n",
    "        \"product\": [],\n",
    "        \"model\": [],\n",
    "        \"date\": [],\n",
    "        \"prediction\": [],\n",
    "    }\n",
    "\n",
    "    # Agrupamos los datos por país y producto para luego iterar sobre los grupos.\n",
    "    grouped = data.groupby([\"country\", \"product\"])\n",
    "\n",
    "    # Iteramos sobre los modelos y los grupos de datos, entrenamos cada modelo para cada grupo.\n",
    "    # Luego hacemos las predicciones y las guardamos en el diccionario.\n",
    "    for model in models:\n",
    "        for (country, product), group in grouped:\n",
    "            forecaster = ForecasterAutoreg(regressor=model, lags=4)\n",
    "            forecaster.fit(y=group[\"sales\"])\n",
    "            predictions = forecaster.predict(steps=prediction_horizon)\n",
    "\n",
    "            # Generamos las fechas de las predicciones\n",
    "            prediction_dates = [\n",
    "                group[\"date\"].max() + MonthBegin(i)\n",
    "                for i in range(1, prediction_horizon + 1)\n",
    "            ]\n",
    "\n",
    "            for date, predicted_sales in zip(prediction_dates, predictions):\n",
    "                predictions[\"country\"].append(country)\n",
    "                predictions[\"product\"].append(product)\n",
    "                predictions[\"model\"].append(type(model).__name__)\n",
    "                predictions[\"date\"].append(date)\n",
    "                predictions[\"prediction\"].append(predicted_sales)\n",
    "\n",
    "    # Convertimos el diccionario en un DataFrame\n",
    "    predictions_dataframe = pd.DataFrame(predictions)\n",
    "\n",
    "    return predictions_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274fb3bc",
   "metadata": {},
   "source": [
    "Voy a suponer que tienes los datos de todos los países y productos en el mismo DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8504bc87",
   "metadata": {},
   "source": [
    "Esta es la función en la que se ejecuta `train_predict()` de forma paralela. Tienes dos opciones:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e8ab8c",
   "metadata": {},
   "source": [
    "- **Definir manualmente los grupos**: lo ideal es que los grupos estén lo más balanceados posible en términos de volumen de datos, por lo que si sabes que hay países que tienen más productos, puedes pensar en qué países agrupar juntos para que todos los grupos tengan más o menos el mismo número de datos. Ten en cuenta que los grupos tampoco pueden ser muy pequeños porque entonces puede que no merezca la pena paralelizar. En este caso le pasamos una lista de DataFrames, en el que cada DataFrame es un grupo que ya has creado previamente (o puedes dividirlos dentro de este función, como veas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dde41d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_train_manual(input_data: list[pd.DataFrame], models: list, n_cores: int) -> pd.DataFrame:\n",
    "    \"\"\"Trains models on multiple dataframes in parallel using the specified number of cores.\n",
    "\n",
    "    Args:\n",
    "        input_data: A list of DataFrames to train the models on.\n",
    "        models: A list of models to be trained.\n",
    "        n_cores: The number of cores to use for parallel processing.\n",
    "\n",
    "    Returns:\n",
    "        A DataFrame containing the predictions from all models and dataframes.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If the number of dataframes does not match the number of cores.\n",
    "    \"\"\"\n",
    "    # Primero comprobamos que el número de dataframes sea igual al número de cores.\n",
    "    assert len(input_data) == n_cores, \"The number of dataframes must match the number of cores.\"\n",
    "\n",
    "    # Luego usamos la Parallel de joblib para entrenar los modelos en paralelo.\n",
    "    predictions = Parallel(n_jobs=n_cores)(delayed(train_predict)(models, data) for data in input_data)\n",
    "\n",
    "    # Finalmente concatenamos los resultados en un solo DataFrame.\n",
    "    predictions_dataframe = pd.concat([result for result in predictions])\n",
    "    return predictions_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd367eb2",
   "metadata": {},
   "source": [
    "- **Dividir los grupos en base al número de cores que quieres utilizar**: esta opción es más \"automática\" pero también puede ser más problemática ya que pueden crearse grupos demasiado pequeños. Si siempre tienes los mismos países y casi todos los países tienen más o menos el mismo volumen de datos, puedes decantarte por esta opción. En este caso se crean los grupos en base al número de cores que definas, simplemente dividiendo la lista de países entre el número de cores y creando una lista de dataframes filtrados por grupo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a0bf61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_train_auto(input_data: pd.DataFrame, models: list, n_cores: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Automatically groups the input data by country and trains models on these groups\n",
    "    in parallel using the specified number of cores.\n",
    "\n",
    "    Args:\n",
    "        input_data: Input DataFrame to train the models on.\n",
    "        models: A list of models to be trained.\n",
    "        n_cores: The number of cores to use for parallel processing.\n",
    "\n",
    "    Returns:\n",
    "        A DataFrame containing the predictions from all models and groups.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If the number of groups does not match the number of cores.\n",
    "    \"\"\"\n",
    "    # Dividimos el DataFrame en grupos por país, en base al número de cores,\n",
    "    # mapeamos cada país a un grupo para que todos los productos de un mismo país\n",
    "    # siempre estén en el mismo grupo.\n",
    "    country_list = input_data[\"country\"].unique()\n",
    "    country_to_group = {country: i % n_cores for i, country in enumerate(country_list)}\n",
    "    input_data[\"group\"] = input_data[\"country\"].map(country_to_group)\n",
    "    dataframes = [group.drop(columns=\"group\") for _, group in input_data.groupby(\"group\")]\n",
    "\n",
    "    # Comprobamos que el número de grupos sea igual al número de cores.\n",
    "    assert len(dataframes) == n_cores, \"The number of dataframes must match the number of cores.\"\n",
    "\n",
    "    # Luego usamos Parallel de joblib para entrenar los modelos en paralelo.\n",
    "    predictions = Parallel(n_jobs=n_cores)(delayed(train_predict)(models, data) for data in dataframes)\n",
    "\n",
    "    # Finalmente concatenamos los resultados en un solo DataFrame.\n",
    "    predictions_dataframe = pd.concat([result for result in predictions])\n",
    "    return predictions_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd2dd73",
   "metadata": {},
   "source": [
    "Mi recomendación es que inicialmente definas manualmente los grupos, puedes empezar con 2 o 4 grupos, agrupas los países para que queden más o menos balanceados, si te funciona bien puedes ir probando a hacer más grupos hasta que veas que ya no te merece la pena añadir más cores. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f29257",
   "metadata": {},
   "source": [
    "Comprueba por si acaso cuántos cores tienes disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "832c8d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available cores: 12\n"
     ]
    }
   ],
   "source": [
    "from joblib import cpu_count\n",
    "\n",
    "print(f'Number of available cores: {cpu_count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4396d38",
   "metadata": {},
   "source": [
    "### Notas\n",
    "\n",
    "- Lo ideal es que muevas el preprocesamiento de los datos fuera de la paralelización, realizando el procesamiento con `pyspark`, y en la función de entrenamiento ya metas los datos listos para entrenar.\n",
    "- Para los modelos que acepten `n_jobs` es mejor fijarlo en 1 para evitar problemas cuando se usan muchos cores (sobretodo `lightgbm`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
